{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from app.config.config import settings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=settings.zhipu_api_key,\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:00:58.460269Z",
     "start_time": "2024-08-15T07:00:57.902586Z"
    }
   },
   "id": "4f31fcba1695bd9e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，Huison！有什么我可以帮助你的吗？如果有任何问题或需要协助的事项，请随时告诉我。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res = llm.invoke([HumanMessage(content=\"我是huison\")])\n",
    "print(res.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T01:24:22.764378Z",
     "start_time": "2024-08-15T01:24:21.288865Z"
    }
   },
   "id": "a4c0b9aa336d5343",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作为人工智能助手，我没有办法知道您的名字，除非您告诉我。如果您愿意，可以告诉我您的名字，我会很高兴认识您。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res1 = llm.invoke([HumanMessage(content=\"我叫什么名字?\")])\n",
    "print(res1.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T01:24:24.915518Z",
     "start_time": "2024-08-15T01:24:23.638680Z"
    }
   },
   "id": "448f268254ff105d",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='你好，Huison！既然你告诉我你的名字是Huison，那么我就这样称呼你了。有什么我可以帮助你的吗？', response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 14, 'total_tokens': 42}, 'model_name': 'glm-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8b16e434-3258-4144-b56c-200f8ac8b8eb-0', usage_metadata={'input_tokens': 14, 'output_tokens': 28, 'total_tokens': 42})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke([HumanMessage(content=\"我是huison\"), HumanMessage(content=\"我叫什么名字?\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T01:26:43.963894Z",
     "start_time": "2024-08-15T01:26:42.637886Z"
    }
   },
   "id": "1dfdc6bbbdbdea8c",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:01:01.878992Z",
     "start_time": "2024-08-15T07:01:01.867516Z"
    }
   },
   "id": "e3f7e680a7006777",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:01:31.922931Z",
     "start_time": "2024-08-15T07:01:31.918650Z"
    }
   },
   "id": "94158f4bdec98c8e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='明白了，Huison。既然你是Huison，请问有什么我可以为你做的？如果你需要帮助或者有任何问题，随时提出。', response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 37, 'total_tokens': 65}, 'model_name': 'glm-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-530a943b-8d16-449d-b098-4a88c18d50b8-0', usage_metadata={'input_tokens': 37, 'output_tokens': 28, 'total_tokens': 65})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"我是huison\")],\n",
    "    config=config,\n",
    ")\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:04:01.806513Z",
     "start_time": "2024-08-15T07:04:00.349306Z"
    }
   },
   "id": "f951654bd5b3f879",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": "'你告诉我你的名字是Huison。如果有其他问题，我会尽力帮助你。'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"我叫什么名字?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:05:05.007598Z",
     "start_time": "2024-08-15T07:05:03.681278Z"
    }
   },
   "id": "d3a0a69b38b00db1",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": "'作为人工智能助手，我没有办法知道您的名字，除非您告诉我。如果您愿意，可以告诉我您的名字，我会很高兴认识您。'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"我叫什么名字?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:32:19.465844Z",
     "start_time": "2024-08-15T07:32:17.955018Z"
    }
   },
   "id": "1618e2b5cd65255b",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": "'You told me your name is Huison. If you have any other questions or need assistance, feel free to ask!'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:33:11.383249Z",
     "start_time": "2024-08-15T07:33:09.963733Z"
    }
   },
   "id": "852b0608347ed18b",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:46:48.298348Z",
     "start_time": "2024-08-15T07:46:48.289009Z"
    }
   },
   "id": "d3383405712e7fbe",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Hello, Bob! How can I assist you today? If you have any questions or need help with something, feel free to ask.'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! I'm bob\")]})\n",
    "\n",
    "response.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:47:14.183376Z",
     "start_time": "2024-08-15T07:47:12.724655Z"
    }
   },
   "id": "2589203ac7d063e9",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:52:29.247331Z",
     "start_time": "2024-08-15T07:52:29.241518Z"
    }
   },
   "id": "c8f61da2200c55b4",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc5\"}}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:52:33.905511Z",
     "start_time": "2024-08-15T07:52:33.901633Z"
    }
   },
   "id": "4d3525cbdc109a88",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Hello, Jim! I'm here to assist you. How can I help you today? If you have any questions or need information on a topic, feel free to ask!\""
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Jim\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:52:40.799550Z",
     "start_time": "2024-08-15T07:52:39.064507Z"
    }
   },
   "id": "cb7ed5493efa0ad7",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"You mentioned that your name is Jim. If you're asking for confirmation, yes, you said your name is Jim. If you're asking in a different context or as a joke, I'll need more information to provide an appropriate response.\""
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:52:54.187190Z",
     "start_time": "2024-08-15T07:52:52.117695Z"
    }
   },
   "id": "ed9a10b2ff67fd33",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:55:52.431423Z",
     "start_time": "2024-08-15T07:55:52.425914Z"
    }
   },
   "id": "2032744a79ac98d8",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'你好，Bob！有什么我可以帮助你的吗？'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"language\": \"Chinese\"}\n",
    ")\n",
    "\n",
    "response.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:56:03.836714Z",
     "start_time": "2024-08-15T07:56:02.907679Z"
    }
   },
   "id": "8ee4d866bba6ca76",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:57:32.622483Z",
     "start_time": "2024-08-15T07:57:32.616364Z"
    }
   },
   "id": "71a9637234e769ef",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:57:40.744750Z",
     "start_time": "2024-08-15T07:57:40.740645Z"
    }
   },
   "id": "5fdd97577a855187",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": "'你好，托德！有什么我可以帮助你的吗？'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"Chinese\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:57:56.070996Z",
     "start_time": "2024-08-15T07:57:54.997692Z"
    }
   },
   "id": "b90e9e9c752038e",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": "'你的名字是Todd。如果你想测试我，请记住，我之前已经告诉你了你的名字是Todd。如果有其他问题，我会很乐意帮助你。'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Chinese\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:59:08.665161Z",
     "start_time": "2024-08-15T07:59:07.219125Z"
    }
   },
   "id": "8ece72b718071414",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 26\u001B[0m\n\u001B[1;32m      3\u001B[0m trimmer \u001B[38;5;241m=\u001B[39m trim_messages(\n\u001B[1;32m      4\u001B[0m     max_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m65\u001B[39m,\n\u001B[1;32m      5\u001B[0m     strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlast\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     start_on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     12\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     13\u001B[0m     SystemMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mre a good assistant\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     14\u001B[0m     HumanMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhi! I\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm bob\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m     AIMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myes!\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     24\u001B[0m ]\n\u001B[0;32m---> 26\u001B[0m trimmer\u001B[38;5;241m.\u001B[39minvoke(messages)\n",
      "File \u001B[0;32m/home/huison/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/runnables/base.py:4475\u001B[0m, in \u001B[0;36mRunnableLambda.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   4461\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001B[39;00m\n\u001B[1;32m   4462\u001B[0m \n\u001B[1;32m   4463\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4472\u001B[0m \u001B[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001B[39;00m\n\u001B[1;32m   4473\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 4475\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_with_config(\n\u001B[1;32m   4476\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_invoke,\n\u001B[1;32m   4477\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   4478\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config(config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc),\n\u001B[1;32m   4479\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4480\u001B[0m     )\n\u001B[1;32m   4481\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   4482\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   4483\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke a coroutine function synchronously.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4484\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse `ainvoke` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4485\u001B[0m     )\n",
      "File \u001B[0;32m/home/huison/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/runnables/base.py:1785\u001B[0m, in \u001B[0;36mRunnable._call_with_config\u001B[0;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[1;32m   1781\u001B[0m     context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[1;32m   1782\u001B[0m     context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, child_config)\n\u001B[1;32m   1783\u001B[0m     output \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m   1784\u001B[0m         Output,\n\u001B[0;32m-> 1785\u001B[0m         context\u001B[38;5;241m.\u001B[39mrun(\n\u001B[1;32m   1786\u001B[0m             call_func_with_variable_args,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m             func,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1788\u001B[0m             \u001B[38;5;28minput\u001B[39m,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1789\u001B[0m             config,\n\u001B[1;32m   1790\u001B[0m             run_manager,\n\u001B[1;32m   1791\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   1792\u001B[0m         ),\n\u001B[1;32m   1793\u001B[0m     )\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1795\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m/home/huison/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/runnables/config.py:427\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    426\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/home/huison/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/runnables/base.py:4331\u001B[0m, in \u001B[0;36mRunnableLambda._invoke\u001B[0;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[1;32m   4329\u001B[0m                 output \u001B[38;5;241m=\u001B[39m chunk\n\u001B[1;32m   4330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4331\u001B[0m     output \u001B[38;5;241m=\u001B[39m call_func_with_variable_args(\n\u001B[1;32m   4332\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc, \u001B[38;5;28minput\u001B[39m, config, run_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m   4333\u001B[0m     )\n\u001B[1;32m   4334\u001B[0m \u001B[38;5;66;03m# If the output is a Runnable, invoke it\u001B[39;00m\n\u001B[1;32m   4335\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, Runnable):\n",
      "File \u001B[0;32m/home/huison/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/runnables/config.py:427\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    426\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/home/huison/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/messages/utils.py:803\u001B[0m, in \u001B[0;36mtrim_messages\u001B[0;34m(messages, max_tokens, token_counter, strategy, allow_partial, end_on, start_on, include_system, text_splitter)\u001B[0m\n\u001B[1;32m    794\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _first_max_tokens(\n\u001B[1;32m    795\u001B[0m         messages,\n\u001B[1;32m    796\u001B[0m         max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    800\u001B[0m         end_on\u001B[38;5;241m=\u001B[39mend_on,\n\u001B[1;32m    801\u001B[0m     )\n\u001B[1;32m    802\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m strategy \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlast\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _last_max_tokens(\n\u001B[1;32m    804\u001B[0m         messages,\n\u001B[1;32m    805\u001B[0m         max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[1;32m    806\u001B[0m         token_counter\u001B[38;5;241m=\u001B[39mlist_token_counter,\n\u001B[1;32m    807\u001B[0m         allow_partial\u001B[38;5;241m=\u001B[39mallow_partial,\n\u001B[1;32m    808\u001B[0m         include_system\u001B[38;5;241m=\u001B[39minclude_system,\n\u001B[1;32m    809\u001B[0m         start_on\u001B[38;5;241m=\u001B[39mstart_on,\n\u001B[1;32m    810\u001B[0m         end_on\u001B[38;5;241m=\u001B[39mend_on,\n\u001B[1;32m    811\u001B[0m         text_splitter\u001B[38;5;241m=\u001B[39mtext_splitter_fn,\n\u001B[1;32m    812\u001B[0m     )\n\u001B[1;32m    813\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    814\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstrategy\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m. Supported strategies are \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlast\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirst\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    816\u001B[0m     )\n",
      "File \u001B[0;32m/home/huison/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/messages/utils.py:918\u001B[0m, in \u001B[0;36m_last_max_tokens\u001B[0;34m(messages, max_tokens, token_counter, text_splitter, allow_partial, include_system, start_on, end_on)\u001B[0m\n\u001B[1;32m    915\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    916\u001B[0m     reversed_ \u001B[38;5;241m=\u001B[39m messages[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 918\u001B[0m reversed_ \u001B[38;5;241m=\u001B[39m _first_max_tokens(\n\u001B[1;32m    919\u001B[0m     reversed_,\n\u001B[1;32m    920\u001B[0m     max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[1;32m    921\u001B[0m     token_counter\u001B[38;5;241m=\u001B[39mtoken_counter,\n\u001B[1;32m    922\u001B[0m     text_splitter\u001B[38;5;241m=\u001B[39mtext_splitter,\n\u001B[1;32m    923\u001B[0m     partial_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlast\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m allow_partial \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    924\u001B[0m     end_on\u001B[38;5;241m=\u001B[39mstart_on,\n\u001B[1;32m    925\u001B[0m )\n\u001B[1;32m    926\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m swapped_system:\n\u001B[1;32m    927\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m reversed_[:\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m reversed_[\u001B[38;5;241m1\u001B[39m:][::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m/home/huison/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/messages/utils.py:833\u001B[0m, in \u001B[0;36m_first_max_tokens\u001B[0;34m(messages, max_tokens, token_counter, text_splitter, partial_strategy, end_on)\u001B[0m\n\u001B[1;32m    831\u001B[0m idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(messages)):\n\u001B[0;32m--> 833\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m token_counter(messages[:\u001B[38;5;241m-\u001B[39mi] \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;28;01melse\u001B[39;00m messages) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m max_tokens:\n\u001B[1;32m    834\u001B[0m         idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(messages) \u001B[38;5;241m-\u001B[39m i\n\u001B[1;32m    835\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/home/huison/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:842\u001B[0m, in \u001B[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001B[0;34m(self, messages)\u001B[0m\n\u001B[1;32m    840\u001B[0m     tokens_per_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    841\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 842\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m    843\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    844\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    845\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[1;32m    846\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for information on how messages are converted to tokens.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    847\u001B[0m     )\n\u001B[1;32m    848\u001B[0m num_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    849\u001B[0m messages_dict \u001B[38;5;241m=\u001B[39m [_convert_message_to_dict(m) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m messages]\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages, AIMessage\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T08:06:34.092215Z",
     "start_time": "2024-08-15T08:06:33.983824Z"
    }
   },
   "id": "edb2f19dec60a909",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todd|，|给你|讲|一个|笑话|吧|：\n",
      "\n",
      "有一天|，|一只|蚂蚁|在|森林|里|散步|，|突然|它|遇到了|一只|被|翻|到|背|面的|乌龟|。\n",
      "蚂蚁|担心|地问|：“|乌龟|先生|，|你怎么|了|？|需要|帮忙|吗|？”|\n",
      "乌龟|无奈|地|回答|：“|哎|，|我|昨天|晚上|参加|了一个|翻身|大赛|，|现在我|还在|等|裁判|来|判|我|是否|翻身|成功|。”||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"hi! I'm todd. tell me a joke\")],\n",
    "        \"language\": \"Chinese\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T08:08:23.203339Z",
     "start_time": "2024-08-15T08:08:20.393131Z"
    }
   },
   "id": "539e277b09d266fc",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "44797acdbb61d7a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
